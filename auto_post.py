import os
import json
import time
import random
import datetime
import requests
import feedparser
from bs4 import BeautifulSoup

# ================== C·∫§U H√åNH CHUNG ==================
PAGE_ID = os.getenv("PAGE_ID", "").strip()
PAGE_ACCESS_TOKEN = os.getenv("PAGE_ACCESS_TOKEN", "").strip()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "").strip()

# S·ªë b√†i t·ªëi ƒëa s·∫Ω ƒëƒÉng trong m·ªói l·∫ßn ch·∫°y (ƒë·ªÉ tr√°nh spam)
MAX_POSTS_PER_RUN = int(os.getenv("MAX_POSTS_PER_RUN", "3"))

# B·∫≠t/t·∫Øt delay gi·ªØa c√°c b√†i (ch·ªâ n√™n b·∫≠t khi ch·∫°y local/server)
ENABLE_DELAY = os.getenv("ENABLE_DELAY", "false").lower() in ("1", "true", "yes")
DELAY_MIN_SEC = int(os.getenv("DELAY_MIN_SEC", str(60 * 60)))       # 1h
DELAY_MAX_SEC = int(os.getenv("DELAY_MAX_SEC", str(2 * 60 * 60)))   # 2h

# T·ª´ kh√≥a l·ªçc b√†i
KEYWORDS = [kw.strip() for kw in os.getenv(
    "KEYWORDS",
    "AI,Artificial Intelligence,Machine Learning,Deep Learning,ChatGPT,Nvidia,OpenAI,LLM,Generative AI"
).split(",") if kw.strip()]

# Ngu·ªìn RSS tin AI (·ªïn ƒë·ªãnh h∆°n so v·ªõi crawl HTML t√πy bi·∫øn)
RSS_FEEDS = [
    # Vi·ªát Nam
    "https://vnexpress.net/rss/so-hoa.rss",
    "https://vietnamnet.vn/rss/khoa-hoc.rss",
    # Qu·ªëc t·∫ø
    "https://www.theverge.com/artificial-intelligence/rss/index.xml",
    "https://venturebeat.com/category/ai/feed/",
    "https://www.wired.com/feed/tag/artificial-intelligence/latest/rss"  # c√≥ th·ªÉ √≠t b√†i
]

LOG_FILE = "post_log.json"

# ================== TI·ªÜN √çCH ==================
def load_log():
    if os.path.exists(LOG_FILE):
        try:
            with open(LOG_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return []
    return []

def save_log(logs):
    with open(LOG_FILE, "w", encoding="utf-8") as f:
        json.dump(logs, f, indent=2, ensure_ascii=False)

def is_already_posted(url, logs):
    for item in logs:
        if item.get("url") == url and item.get("status") == "success":
            return True
    return False

def add_log(title, url, status, extra=None):
    logs = load_log()
    entry = {
        "title": title,
        "url": url,
        "status": status,
        "time": datetime.datetime.now().isoformat()
    }
    if extra:
        entry.update(extra)
    logs.append(entry)
    save_log(logs)

def safe_get(url, timeout=12):
    try:
        return requests.get(url, timeout=timeout, headers={"User-Agent": "Mozilla/5.0"})
    except Exception:
        return None

def extract_og_image(article_url):
    """∆Øu ti√™n ·∫£nh t·ª´ th·∫ª og:image c·ªßa trang b√†i vi·∫øt."""
    resp = safe_get(article_url)
    if not resp or resp.status_code != 200:
        return None
    soup = BeautifulSoup(resp.text, "html.parser")
    og = soup.find("meta", property="og:image")
    if og and og.get("content"):
        return og["content"].strip()
    # fallback: l·∫•y ·∫£nh ƒë·∫ßu ti√™n trong b√†i (n·∫øu c√≥)
    img = soup.find("img")
    if img and (img.get("src") or img.get("data-src")):
        return (img.get("src") or img.get("data-src")).strip()
    return None

def extract_meta_description(article_url):
    """L·∫•y m√¥ t·∫£ ng·∫Øn (ƒë·ªÉ h·ªó tr·ª£ LLM t√≥m t·∫Øt t·ªët h∆°n)."""
    resp = safe_get(article_url)
    if not resp or resp.status_code != 200:
        return ""
    soup = BeautifulSoup(resp.text, "html.parser")
    m = soup.find("meta", attrs={"name": "description"})
    if m and m.get("content"):
        return m["content"].strip()
    og_desc = soup.find("meta", property="og:description")
    if og_desc and og_desc.get("content"):
        return og_desc["content"].strip()
    # fallback: l·∫•y v√†i d√≤ng text ƒë·∫ßu
    text = soup.get_text(separator=" ", strip=True)
    return text[:500]

def contains_keyword(title, summary):
    blob = f"{title} {summary}".lower()
    return any(kw.lower() in blob for kw in KEYWORDS)

def unique_by_url(items):
    seen = set()
    out = []
    for it in items:
        u = it["url"]
        if u not in seen:
            seen.add(u)
            out.append(it)
    return out

# ================== L·∫§Y TIN T·ª™ RSS ==================
def fetch_from_feeds():
    print("üì∞ ƒêang l·∫•y tin t·ª´ RSS...")
    articles = []
    for feed_url in RSS_FEEDS:
        try:
            feed = feedparser.parse(feed_url)
            for e in feed.entries[:10]:  # t·ªëi ƒëa 10 b√†i/ngu·ªìn
                title = (getattr(e, "title", "") or "").strip()
                link = (getattr(e, "link", "") or "").strip()
                summary = (getattr(e, "summary", "") or "").strip()
                if not title or not link:
                    continue
                articles.append({"title": title, "url": link, "summary": summary})
        except Exception as ex:
            print(f"‚ö†Ô∏è L·ªói ƒë·ªçc feed {feed_url}: {ex}")
    articles = unique_by_url(articles)
    print(f"üîé T·ªïng thu th·∫≠p: {len(articles)} b√†i.")
    return articles

# ================== GEMINI: SINH CAPTION ==================
def generate_caption_with_gemini(title, url, context_text):
    if not GEMINI_API_KEY:
        # fallback khi kh√¥ng c√≥ API key
        base = f"{title}\n\nƒê·ªçc th√™m: {url}"
        hashtags = " ".join(sorted({f"#{kw.replace(' ','')}" for kw in ["AI","Tech"]}))
        return f"{base}\n\n{hashtags}"

    try:
        import google.generativeai as genai
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel("gemini-1.5-flash")

        # Chu·∫©n b·ªã hashtag t·ª´ KEYWORDS (g·ªçn t·ªëi ƒëa 3-5 c√°i)
        core_tags = ["AI", "Tech", "MachineLearning", "DeepLearning", "ChatGPT", "Nvidia"]
        chosen = []
        lower_blob = (title + " " + context_text).lower()
        for tag in core_tags:
            if tag.lower() in lower_blob and len(chosen) < 5:
                chosen.append(tag)
        if not chosen:
            chosen = ["AI", "Tech"]
        hashtags = " ".join(f"#{t}" for t in chosen)

        prompt = (
            "B·∫°n l√† bi√™n t·∫≠p vi√™n m·∫°ng x√£ h·ªôi. H√£y vi·∫øt caption Facebook ng·∫Øn g·ªçn (2‚Äì3 c√¢u), "
            "r√µ r√†ng, h·∫•p d·∫´n, t√≥m t·∫Øt n·ªôi dung b√†i vi·∫øt d∆∞·ªõi ƒë√¢y. Kh√¥ng qu√° 600 k√Ω t·ª±. "
            "Kh√¥ng th√™m d·∫•u ngo·∫∑c k√©p ngo√†i c√πng. K·∫øt th√∫c b·∫±ng c√°c hashtag ƒë√£ g·ª£i √Ω.\n\n"
            f"Ti√™u ƒë·ªÅ: {title}\n"
            f"M√¥ t·∫£: {context_text[:1200]}\n"
            f"Link: {url}\n"
            f"Hashtags: {hashtags}\n"
        )
        resp = model.generate_content(prompt)
        text = (resp.text or "").strip()
        if not text:
            raise ValueError("Empty caption from Gemini")
        # ƒë·∫£m b·∫£o c√≥ link
        if "http" not in text:
            text = f"{text}\n\nƒê·ªçc th√™m: {url}"
        # ƒë·∫£m b·∫£o c√≥ hashtags
        if "#" not in text:
            text = f"{text}\n\n{hashtags}"
        return text
    except Exception as e:
        print(f"‚ö†Ô∏è Gemini l·ªói: {e}")
        # fallback
        base = f"{title}\n\nƒê·ªçc th√™m: {url}"
        fallback_tags = " ".join(["#AI", "#Tech"])
        return f"{base}\n\n{fallback_tags}"

# ================== ƒêƒÇNG L√äN FACEBOOK ==================
def post_text(message):
    url = f"https://graph.facebook.com/{PAGE_ID}/feed"
    r = requests.post(url, data={"message": message, "access_token": PAGE_ACCESS_TOKEN})
    return r

def post_photo(image_url, caption):
    url = f"https://graph.facebook.com/{PAGE_ID}/photos"
    r = requests.post(url, data={"url": image_url, "caption": caption, "access_token": PAGE_ACCESS_TOKEN})
    return r

def publish_article(article):
    title = article["title"]
    url = article["url"]

    # L·∫•y m√¥ t·∫£ + ·∫£nh cho b√†i
    meta_desc = article.get("summary") or extract_meta_description(url) or ""
    image_url = extract_og_image(url)

    # Sinh caption b·∫±ng Gemini
    caption = generate_caption_with_gemini(title, url, meta_desc)

    # ƒêƒÉng b√†i
    if image_url:
        resp = post_photo(image_url, caption)
    else:
        resp = post_text(caption)

    if resp.ok:
        print(f"‚úÖ ƒêƒÉng th√†nh c√¥ng: {title}")
        add_log(title, url, "success", {"response": safe_json(resp)})
        return True
    else:
        print(f"‚ùå L·ªói khi ƒëƒÉng: {resp.text}")
        add_log(title, url, "error", {"response": resp.text})
        return False

def safe_json(resp):
    try:
        return resp.json()
    except Exception:
        return {"text": resp.text[:500]}

# ================== LU·ªíNG CH√çNH ==================
def main():
    # Ki·ªÉm tra c·∫•u h√¨nh
    if not PAGE_ID or not PAGE_ACCESS_TOKEN:
        print("‚ùå Thi·∫øu PAGE_ID ho·∫∑c PAGE_ACCESS_TOKEN (bi·∫øn m√¥i tr∆∞·ªùng).")
        return

    print("üì∞ B·∫Øt ƒë·∫ßu l·∫•y tin v√† ƒëƒÉng‚Ä¶")
    logs = load_log()
    all_articles = fetch_from_feeds()

    # L·ªçc theo t·ª´ kh√≥a
    filtered = [a for a in all_articles if contains_keyword(a["title"], a.get("summary", ""))]
    print(f"üîç Sau l·ªçc t·ª´ kh√≥a: {len(filtered)} b√†i.")

    # Lo·∫°i b·ªè b√†i ƒë√£ ƒëƒÉng tr∆∞·ªõc ƒë√≥
    filtered = [a for a in filtered if not is_already_posted(a["url"], logs)]
    print(f"üßπ Sau khi lo·∫°i tr√πng (ƒë√£ ƒëƒÉng): {len(filtered)} b√†i.")

    # Gi·ªõi h·∫°n s·ªë b√†i ƒëƒÉng m·ªói l∆∞·ª£t
    to_post = filtered[:MAX_POSTS_PER_RUN]
    print(f"üóìÔ∏è S·∫Ω ƒëƒÉng {len(to_post)}/{len(filtered)} b√†i trong l∆∞·ª£t n√†y.")

    for idx, article in enumerate(to_post, start=1):
        print(f"‚û°Ô∏è  [{idx}/{len(to_post)}] {article['title']}")
        publish_article(article)

        # Delay ng·∫´u nhi√™n n·∫øu b·∫≠t
        if ENABLE_DELAY and idx < len(to_post):
            delay = random.randint(DELAY_MIN_SEC, DELAY_MAX_SEC)
            print(f"‚è≥ Ch·ªù {delay//60} ph√∫t tr∆∞·ªõc b√†i k·∫ø ti·∫øp‚Ä¶")
            time.sleep(delay)

    print("üèÅ Ho√†n t·∫•t.")

if __name__ == "__main__":
    main()
